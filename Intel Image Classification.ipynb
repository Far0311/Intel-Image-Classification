{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f85ba52f-2d10-4c65-9ce8-6df5ea5b8292",
   "metadata": {},
   "source": [
    "# 1.0 Problem Definition and Dataset Selection\n",
    "\n",
    "## 1.1 Problem Definition\n",
    "\n",
    "Image classification is a fundamental task in computer vision with widespread applications, including scene recognition, surveillance, and autonomous navigation. This project addresses the challenge of **automatically classifying natural scenes into six categories**: buildings, forests, glaciers, mountains, sea, and street. Manual classification of such images can be time-consuming and inconsistent, especially when dealing with large volumes of visual data. A deep learning-based classification model can provide efficient and scalable solutions for automated image tagging and analysis.\n",
    "\n",
    "Scene image classification plays a vital role across various domains, including environmental monitoring, urban planning, and content-based image retrieval. From my professional experience, non-profit organizations such as [Earthworm Foundation](https://earthworm.org/?gad_source=1&gad_campaignid=12129170521&gbraid=0AAAAABo_BusCN3Zb3m82RmJjKYadQmE6w&gclid=CjwKCAjwyb3DBhBlEiwAqZLe5NSulkb6NxMQy0a5uMmaP4UkF22-sYrwWQiSWl-h2lP2K_a7yyVDKxoCmEkQAvD_BwE), [Global Forest Watch](https://www.globalforestwatch.org/), and the [World Wide Fund for Nature](https://www.wwf.org.my/) often depend on manual interpretation of satellite imagery for monitoring purposes. While this method can be effective, it is labor-intensive and prone to human error, underscoring the need for automated and scalable approaches. This study aims to evaluate multiple deep learning models to determine the most effective architecture for accurately distinguishing between different scene types based on their visual features.\n",
    "\n",
    "## 1.2 Dataset Selection\n",
    "\n",
    "The **Intel Image Classification dataset**, available on [Kaggle](https://www.kaggle.com/datasets/puneet6060/intel-image-classification), has been selected for this study. It contains over **25,000 labeled RGB images**, each resized to 150×150 pixels, categorized into six scene classes: buildings, forests, glaciers, mountains, sea, and street. The dataset is organized into separate training, testing, and prediction sets, making it convenient for supervised learning tasks.\n",
    "\n",
    "This dataset is well-suited for the problem due to the following reasons:\n",
    "- It provides a balanced distribution of images across all six classes, minimizing the risk of class imbalance.\n",
    "- The image categories represent a variety of natural and urban environments, making the classification task non-trivial and realistic.\n",
    "- The resolution and size of the images are compatible with most convolutional neural network (CNN) architectures without the need for extensive computational resources.\n",
    "- The dataset's relevance to real-world applications enhances the practical value of the model evaluation.\n",
    "\n",
    "Overall, the Intel dataset offers a reliable and well-structured benchmark for developing and comparing deep learning models for scene classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0f560d-bcee-45d4-8c2b-ed233d9bc23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f25aa71-38a3-4ea0-a911-e2ffdc6c6e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the root directories\n",
    "train_dir = r\"C:\\Users\\user\\OneDrive - Universiti Teknologi Malaysia (UTM)\\MRTB2153 Advanced Artificial Intelligence\\PMA\\Intel Image Classification\\seg_train\\seg_train\"\n",
    "test_dir = r\"C:\\Users\\user\\OneDrive - Universiti Teknologi Malaysia (UTM)\\MRTB2153 Advanced Artificial Intelligence\\PMA\\Intel Image Classification\\seg_test\\seg_test\"\n",
    "\n",
    "# Define image transformations\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load datasets using ImageFolder\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform_train)\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform=transform_test)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Get class names\n",
    "class_names = train_dataset.classes\n",
    "print(f\"Class names: {class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f01275c-3a4c-4225-b524-701d0a982cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a088802e-3999-4d34-b763-8c6440e3ba22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to unnormalize and show image\n",
    "def imshow(img):\n",
    "    img = img.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    img = std * img + mean  # unnormalize\n",
    "    img = np.clip(img, 0, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Display a batch of training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "print(f\"Image tensor shape: {images.shape}\")  # [batch_size, channels, height, width]\n",
    "imshow(images[0])  # Show first image in the batch\n",
    "print(f\"Label: {class_names[labels[0]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa0dc9b-88ef-4bff-9a40-b52aa5e191cb",
   "metadata": {},
   "source": [
    "## 2.0 Methodology\n",
    "\n",
    "This section outlines the end-to-end approach used to develop, tune, and evaluate image classification models for the Intel Image Classification dataset. The workflow is divided into several key stages:\n",
    "\n",
    "---\n",
    "\n",
    "### 2.1 Data Preprocessing\n",
    "\n",
    "The dataset used is the **Intel Image Classification** dataset, consisting of six classes: `buildings`, `forest`, `glacier`, `mountain`, `sea`, and `street`. It is split into three folders:\n",
    "\n",
    "- `seg_train`: Labelled images used for training and validation\n",
    "- `seg_test`: Labelled images used to evaluate final model performance\n",
    "- `seg_pred`: Unlabelled images used for inference\n",
    "\n",
    "The following preprocessing steps were applied:\n",
    "\n",
    "- **Resize**: All images were resized to 150x150 pixels.\n",
    "- **Normalization**: Pixel values were normalized using ImageNet mean and standard deviation.\n",
    "- **Augmentation** (training set only): Horizontal flipping and small random rotations were applied to improve generalization.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 Model Selection\n",
    "\n",
    "Three architectures were implemented and compared:\n",
    "\n",
    "- **Simple CNN**: A custom convolutional neural network with tunable dropout.\n",
    "- **ResNet50**: A pre-trained residual network from PyTorch’s `torchvision.models`.\n",
    "- **EfficientNet-B0**: A lightweight yet powerful image classification model.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.3 Hyperparameter Tuning with Optuna\n",
    "\n",
    "To optimize each model’s performance, **Optuna** was used for automated hyperparameter tuning. For each model, a search space was defined, and trials were run to identify the best combination of:\n",
    "\n",
    "- Learning rate (`lr`)\n",
    "- Weight decay\n",
    "- Dropout rate (for CNN)\n",
    "- Optimizer (`Adam` or `SGD`)\n",
    "- Batch size\n",
    "- Momentum (for SGD-based optimizers)\n",
    "\n",
    "The objective function was based on validation accuracy, and trials were tracked using Optuna’s `study.optimize`.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.4 Training Procedure\n",
    "\n",
    "For each model, the following training pipeline was followed:\n",
    "\n",
    "- **Loss Function**: CrossEntropyLoss was used for multi-class classification.\n",
    "- **Optimizers**: Optimizer selection (Adam or SGD) was based on Optuna tuning.\n",
    "- **Epochs**: All models were trained for 10 epochs.\n",
    "- **Device**: Training was conducted on GPU.\n",
    "\n",
    "Additionally:\n",
    "\n",
    "- For **ResNet50** and **EfficientNet-B0**, feature extraction was used by freezing the pretrained base layers and fine-tuning the classifier head.\n",
    "- For **CNN**, dropout was applied to prevent overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.5 Evaluation and Inference\n",
    "\n",
    "- **Metrics**: Models were evaluated using precision, recall, accuracy, F1-score and confusion matrices on the `seg_test` dataset.\n",
    "- **Loss Curves**: Training and validation loss were plotted to monitor learning behavior.\n",
    "- **Class Distribution Analysis**: Predictions on the unlabelled `seg_pred` images were analyzed for each model to assess class balance and visual accuracy.\n",
    "- **Qualitative Analysis**: Sample predictions were manually reviewed to identify strengths and weaknesses in classifying confusing scenes (urban or snowy landscapes).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9612e023-9b0b-4882-aa6f-c951ec2f6252",
   "metadata": {},
   "source": [
    "# 3.0 Data Preprocessing and Exploration\n",
    "\n",
    "## 3.1 Exploratory Data Analysis (EDA)\n",
    "\n",
    "To begin the analysis, the dataset was explored to understand its structure, class distribution, and image properties. The training dataset (`seg_train`) and testing dataset (`seg_test`) are organized into six subfolders representing the classes: **buildings**, **forest**, **glacier**, **mountain**, **sea**, and **street**. This folder structure is compatible with PyTorch's `ImageFolder` for supervised learning tasks.\n",
    "\n",
    "A bar chart of image counts per class reveals a **balanced dataset**, with no significant class imbalance. Sample images were also visualized to observe the variety in visual textures, lighting conditions, and scene content. All images are in color (RGB) and of manageable resolution, enabling efficient training on most deep learning models without resizing overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b13f57-34c6-42f5-ade2-b1a6d3bf734a",
   "metadata": {},
   "source": [
    "### Class Distribution Analysis\n",
    "\n",
    "The bar chart reveals a balanced dataset with all six classes (`buildings`, `forest`, `glacier`, `mountain`, `sea`, and `street`) containing between 2200 and 2500 images each. This eliminates the need for additional sampling strategies and allows the model to train fairly across categories.\n",
    "\n",
    "### Sample Image Inspection\n",
    "\n",
    "Sample images from each class show meaningful variation in visual features such as texture, color, and structure. For example, forests exhibit dense greenery, while glaciers feature high-contrast icy terrain. This diversity supports the relevance and complexity of the classification task, indicating that the model will need to extract meaningful spatial and contextual features to distinguish between categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405794d2-6456-4494-a647-8fd25a3d5eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbd3df1-5ce4-44e1-8b9c-3f33ef674b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count images per class\n",
    "class_counts = {cls: len(os.listdir(os.path.join(train_dir, cls))) for cls in os.listdir(train_dir)}\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x=list(class_counts.keys()), y=list(class_counts.values()))\n",
    "plt.title(\"Class Distribution in Training Set\")\n",
    "plt.ylabel(\"Number of Images\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Show sample images from each class\n",
    "fig, axs = plt.subplots(2, 3, figsize=(12, 6))\n",
    "for idx, cls in enumerate(class_counts.keys()):\n",
    "    img_path = os.path.join(train_dir, cls, os.listdir(os.path.join(train_dir, cls))[0])\n",
    "    img = Image.open(img_path)\n",
    "    axs[idx//3, idx%3].imshow(img)\n",
    "    axs[idx//3, idx%3].set_title(cls)\n",
    "    axs[idx//3, idx%3].axis('off')\n",
    "plt.suptitle(\"Sample Images from Each Class\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad94eb8d-5500-4e0f-b81c-55b7c0e30121",
   "metadata": {},
   "source": [
    "## 3.2 Preprocessing Steps\n",
    "\n",
    "To prepare the data for training deep learning models, the following preprocessing steps were applied:\n",
    "\n",
    "- **`Resize`**: All images were resized to 150×150 pixels to ensure consistent input dimensions across the model.\n",
    "- **`Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]`**: Pixel values were normalized using ImageNet mean and standard deviation to accelerate convergence during training and ensure numerical stability. This ensures the input distribution matches what many pretrained CNN models expect, facilitating more stable and effective training.\n",
    "- **Data Augmentation** via `RandomHorizontalFlip()` and `RandomRotation(15)`: Random horizontal flipping and slight rotation were introduced in the training pipeline to improve generalization and reduce overfitting. The `RandomRotation(15)` transformation introduces a small degree of variation (±15°) in the training images, helping the model become more robust to slight changes in camera angle or perspective. The 15° range is large enough to promote generalization, yet small enough to preserve the original scene semantics.\n",
    "- **Tensor Conversion** using `ToTensor()`: Images were converted from PIL format to PyTorch tensors for model consumption.\n",
    "- **Dataset Splitting**: The dataset was already separated into training (`seg_train`), testing (`seg_test`), and prediction (`seg_pred`) folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58267691-46c5-4f7f-9c8f-a77553c7a4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2788424e-9d59-4642-9d0a-7521e9f99c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b43646-72d5-45d9-b199-643358a3a61b",
   "metadata": {},
   "source": [
    "## 3.3 Handling Data Imbalance or Missing Data\n",
    "A check was conducted to ensure the dataset was not affected by class imbalance. Each of the six scene categories contained a comparable number of images, making it suitable for training without additional oversampling or undersampling techniques.\n",
    "\n",
    "Additionally, the dataset was scanned for **corrupted or unreadable image files**. No such images were found. A try-except block was implemented during data loading to verify image integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b53a45-ebd3-427d-9f57-64399954e4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26b4d31-7af5-4807-a732-4dcfcfe0c1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupted = []\n",
    "for cls in os.listdir(train_dir):\n",
    "    for img_file in os.listdir(os.path.join(train_dir, cls)):\n",
    "        try:\n",
    "            img = Image.open(os.path.join(train_dir, cls, img_file))\n",
    "            img.verify()\n",
    "        except:\n",
    "            corrupted.append((cls, img_file))\n",
    "\n",
    "print(f\"Corrupted images found: {len(corrupted)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e86830-1c8e-43c2-bdc2-3f7afd8f348f",
   "metadata": {},
   "source": [
    "## 3.4 Justification of Preprocessing Steps\n",
    "- **Resizing** ensures all images have the same input shape, which is essential for convolutional neural networks (CNNs).\n",
    "- **Normalization** helps stabilize and speed up the training process by bringing pixel values to a standard range.\n",
    "- **Augmentation techniques** such as random flipping and rotation introduce variability into the training set, improving the model’s ability to generalize to unseen data.\n",
    "- **Data splitting** into training and test sets allows the model to be evaluated fairly on unseen examples.\n",
    "- Checking for **missing/corrupt images** prevents training failures due to unreadable files.\n",
    "\n",
    "These preprocessing decisions align with best practices for image classification using deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c194b12-2721-4b58-a356-f822ec14b2ce",
   "metadata": {},
   "source": [
    "# 4.0 Model Implementation\n",
    "## 4.1 Model Selection\n",
    "\n",
    "For the Intel Image Classification task, the following three models were selected based on peer-reviewed literature and their effectiveness in scene classification:\n",
    "\n",
    "### 4.1.1. Convolutional Neural Networks (CNNs)\n",
    "\n",
    "CNNs form the foundation of modern image classification. A custom CNN built from scratch provides insights into how deep models learn hierarchical visual features (edges, textures, patterns) directly from raw image data (Zhou et al., 2014).\n",
    "\n",
    "- CNNs are widely used in remote sensing and scene classification tasks.\n",
    "- They automatically extract translation-invariant features without manual feature engineering.\n",
    "- Useful as a lightweight baseline model for understanding the effects of architectural depth and complexity.\n",
    "\n",
    "*Justification:* CNNs are a core building block for visual scene understanding, and their relevance in scene classification is well-established in recent research.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.1.2. ResNet (Residual Networks)\n",
    "\n",
    "ResNet introduces skip connections to solve vanishing gradients and enables effective training of very deep networks (Shabbir et al., 2021).\n",
    "\n",
    "- ResNet50, pretrained on ImageNet, is widely adopted in scene and satellite image classification.\n",
    "- Fine-tuning allows leveraging high-level features learned from millions of images, accelerating training and boosting accuracy.\n",
    "- ResNet has achieved 92%–99.6% accuracy on related scene image datasets.\n",
    "\n",
    "*Justification:* ResNet50 is a proven architecture in scene classification literature due to its robust performance and excellent transfer learning capabilities.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.1.3. EfficientNet\n",
    "\n",
    "EfficientNet uses compound scaling to balance depth, width, and resolution, achieving high accuracy with fewer parameters and less computation (Tan et al., 2021; Jain et al., 2024).\n",
    "\n",
    "-  EfficientNet achieved an impressive accuracy rate of 94.31% across the dataset.\n",
    "-  EfficientNet has been recognised for reaching \"new highs in the efficiency and accuracy of model architecture\".\n",
    "-  The compound scaling strategy enables high performance with fewer parameters. \n",
    "\n",
    "\n",
    "*Justification:* EfficientNet offers state-of-the-art performance with high computational efficiency, making it ideal for large-scale image scene classification under resource constraints.\n",
    "\n",
    "---\n",
    "\n",
    "The combination of:\n",
    "- A **custom CNN** for foundational understanding,\n",
    "- A **ResNet50** for deep residual learning and transfer learning performance,\n",
    "- An **EfficientNet** for state-of-the-art efficiency and accuracy\n",
    "\n",
    "provides a diverse and well-justified model selection strategy for benchmarking scene classification performance on the Intel dataset.\n",
    "\n",
    "---\n",
    "### 4.1.4 Architectural Comparison: CNN vs ResNet vs EfficientNet\n",
    "\n",
    "| Feature                      | CNN                                          | ResNet                                           | EfficientNet                                     |\n",
    "|-----------------------------|----------------------------------------------|--------------------------------------------------|--------------------------------------------------|\n",
    "| **Core Idea**               | Basic deep learning model for visual data    | Adds residual connections to train deeper models | Uses compound scaling for optimal accuracy/efficiency |\n",
    "| **Main Innovation**         | Hierarchical convolution + pooling layers    | Skip connections (Residual Blocks)               | Compound scaling of depth, width, and resolution |\n",
    "| **Problem Addressed**       | Feature extraction and classification        | Vanishing gradients in deep networks             | Balancing model size and efficiency              |\n",
    "| **Structure**               | Conv → Pool → FC → Softmax                   | Residual blocks with identity mappings           | MBConv + SE blocks + compound scaling            |\n",
    "| **Gradient Flow**           | Gradients pass through each layer sequentially | Skip connections ease gradient flow              | Optimized baseline ensures stable scaling        |\n",
    "| **Scalability**             | Manually add layers                          | Easily scales with deeper blocks                 | Automatically scales all dimensions (d, w, r)    |\n",
    "| **Model Examples**          | Custom CNN, LeNet, AlexNet                   | ResNet18, ResNet50, ResNet101                    | EfficientNet-B0 to B7                            |\n",
    "| **Pretraining Use**         | Optional                                     | Common in transfer learning (ImageNet)           | Designed for pretrained efficiency               |\n",
    "| **Purpose**                 | Foundational model for image tasks           | Enables very deep networks without degradation   | Achieves high accuracy with fewer parameters     |\n",
    "\n",
    "\n",
    "---\n",
    "References:\n",
    "- Jain, R., Sharma, R., Tiwari, D., Joshi, K., & Jain, V. (2024). Enhanced Classification of Intel Images Using Refined EfficientNet and MobileNetV2 Frameworks. In 2023 4th International Conference on Intelligent Technologies (CONIT) (pp. 1-4). IEEE.\n",
    "- Shabbir, A., Ali, N., Ahmed, J., Zafar, B., Rasheed, A., Sajid, M., ... & Dar, S. H. (2021). Satellite and scene image classification based on transfer learning and fine tuning of ResNet50. Mathematical Problems in Engineering, 2021(1), 5843816.\n",
    "- Tan, M., & Le, Q. (2019, May). Efficientnet: Rethinking model scaling for convolutional neural networks. In International conference on machine learning (pp. 6105-6114). PMLR.\n",
    "- Zhou, B., Lapedriza, A., Xiao, J., Torralba, A., & Oliva, A. (2014). Learning deep features for scene recognition using places database. Advances in neural information processing systems, 27."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c637a8f9-d4e7-4a49-b86c-9a377bed51a5",
   "metadata": {},
   "source": [
    "## 4.2 Model Implementation & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74159642-750b-419c-82ab-7bfc65844301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f442bd6-1f62-4570-99f2-dce8cd397d24",
   "metadata": {},
   "source": [
    "### 4.2.1 Model Definition\n",
    "**1. CNN**\n",
    "\n",
    "The `SimpleCNN` model is a custom convolutional neural network designed for image classification with 6 output classes. It consists of three main convolutional blocks followed by fully connected layers:\n",
    "\n",
    "- **Convolutional Layers (`conv1`, `conv2`, `conv3`)**: Each layer applies 2D convolution with ReLU activation and padding to preserve spatial dimensions. These layers progressively learn low- to high-level visual features.\n",
    "- **Max Pooling (`pool`)**: After each convolution, max pooling with a 2×2 kernel downsamples the feature maps, reducing spatial dimensions and computational cost.\n",
    "- **Dropout**: A dropout layer is applied before the final classification layer to prevent overfitting by randomly deactivating neurons during training.\n",
    "- **Fully Connected Layers (`fc1`, `fc2`)**:\n",
    "  - `fc1`: Flattens and reduces features from the last convolutional block to 256 neurons.\n",
    "  - `fc2`: Maps the 256 features to 6 output classes (scene labels).\n",
    "\n",
    "This architecture is relatively shallow and efficient, making it suitable as a baseline model for image classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74615b9-65e6-4760-b07f-15f70441d036",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=6, dropout_rate=0.5):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc1 = nn.Linear(128 * 18 * 18, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 128 * 18 * 18)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd000779-7f13-456e-ac78-259a1cd42659",
   "metadata": {},
   "source": [
    "**2. ResNet50**\n",
    "\n",
    "The `build_resnet50()` function loads a pretrained ResNet50 model and adapts it for the Intel scene classification task (6 classes):\n",
    "\n",
    "- **Pretrained Weights**: The model uses weights pretrained on ImageNet, enabling it to leverage learned visual features.\n",
    "- **Feature Freezing**: All layers are frozen to retain the pretrained feature extractor and reduce training time.\n",
    "- **Classifier Replacement**: The original fully connected layer (`model.fc`) is replaced with a new linear layer with 6 output units to match the number of scene categories.\n",
    "\n",
    "This transfer learning approach allows the model to generalize well to new image classification tasks with limited training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cef672-a0ae-41e3-97a6-1f068e0823e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_resnet50(num_classes=6):\n",
    "    model = models.resnet50(pretrained=True)\n",
    "\n",
    "    # Freeze earlier layers (optional)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Replace the classifier\n",
    "    in_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(in_features, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc291401-73b0-4f7c-a540-b52d018839d1",
   "metadata": {},
   "source": [
    "**3. EfficientNet-B0**\n",
    "\n",
    "The `build_efficientnet_b0()` function loads a pretrained EfficientNet-B0 model and customizes it for the 6-class scene classification task:\n",
    "\n",
    "- **Pretrained Weights**: The model is initialized with weights pretrained on ImageNet, which helps transfer rich feature representations to the new task.\n",
    "- **Feature Extractor Freezing**: All convolutional feature extraction layers are frozen to preserve learned representations and reduce training time.\n",
    "- **Classifier Replacement**: The final classification layer is replaced with a new fully connected layer with 6 output neurons to match the number of scene categories.\n",
    "\n",
    "EfficientNet-B0 is known for its high accuracy and computational efficiency, making it a strong candidate for transfer learning in image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f8688f-4b2c-467e-8fc4-887abc43e772",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_efficientnet_b0(num_classes=6):\n",
    "    model = models.efficientnet_b0(pretrained=True)\n",
    "\n",
    "    # Freeze feature extractor\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Replace classifier\n",
    "    in_features = model.classifier[1].in_features\n",
    "    model.classifier[1] = nn.Linear(in_features, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4a0d52-ee24-4281-8d22-93457c9e3a19",
   "metadata": {},
   "source": [
    "### 4.2.2 Hyperparameter Tuning Strategy\n",
    "\n",
    "`train_dataset` is split into two parts: 80% train and 20% validation using `random_split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e152fa-59cf-4d2c-b388-9feee88caeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# Split train_dataset into train + val\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset_split, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# Create loaders\n",
    "train_loader = DataLoader(train_dataset_split, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd50918-a053-4aec-8aa1-3bf39dfc881e",
   "metadata": {},
   "source": [
    "#### *Optuna Objective Function*\n",
    "\n",
    "The `objective()` function defines the hyperparameter tuning process for Optuna. This function enables automated tuning across different architectures to identify the best-performing configuration. It supports three model types: `SimpleCNN`, `ResNet50`, and `EfficientNet-B0`. The key components are:\n",
    "\n",
    "---\n",
    "- **Hyperparameter Sampling**:\n",
    "Hyperparameter tuning was conducted using Optuna for all three models: **CNN**, **ResNet50**, and **EfficientNet-B0**. \n",
    "\n",
    "    - For **ResNet50** and **EfficientNet-B0**, two hyperparameters were tuned:\n",
    "      - `learning_rate`\n",
    "      - `optimizer`\n",
    "    \n",
    "    - For the **CNN model**, an additional `dropout` parameter was included to help regularize and reduce overfitting, making it three tunable hyperparameters in total.\n",
    "    \n",
    "    The best *1st trial* results for each model are as follows:\n",
    "    \n",
    "    #### Best CNN Trial 1 :\n",
    "    - **Accuracy**: `0.8297`\n",
    "    - **Parameters**:  \n",
    "      - `lr`: `0.000609`  \n",
    "      - `optimizer`: `Adam`  \n",
    "      - `dropout`: `0.5697`\n",
    "    \n",
    "    #### Best ResNet50 Trial 1 :\n",
    "    - **Accuracy**: `0.8828`\n",
    "    - **Parameters**:  \n",
    "      - `lr`: `0.000214`  \n",
    "      - `optimizer`: `Adam`\n",
    "    \n",
    "    #### Best EfficientNet-B0 Trial 1 :\n",
    "    - **Accuracy**: `0.8557`\n",
    "    - **Parameters**:  \n",
    "      - `lr`: `0.001659`  \n",
    "      - `optimizer`: `SGD`\n",
    "\n",
    "    > To further improve the tuning process, **additional hyperparameters** such as `weight_decay`, `momentum` (for SGD), and `batch_size` were later introduced in subsequent tuning iterations:\n",
    "    - **Weight Decay**: Helps prevent overfitting by penalizing large weights.\n",
    "    - **Batch Size**: Affects gradient estimates and memory efficiency.\n",
    "    - **Momentum**: (For SGD only) Accelerates convergence.\n",
    "\n",
    "---\n",
    "\n",
    "- **Model Initialization**:\n",
    "  - Based on the selected `model_type`, the appropriate model is created and transferred to GPU.\n",
    "  - For ResNet and EfficientNet, pretrained weights are used and feature extractor layers are frozen.\n",
    "  - The final classifier layers are replaced with a new layer matching the number of classes (6).\n",
    "\n",
    "---\n",
    "\n",
    "- **Training Loop**:\n",
    "  - Each model is trained for 5 short epochs using the sampled hyperparameters.\n",
    "  - The `CrossEntropyLoss` is used for multi-class classification.\n",
    "---\n",
    "  \n",
    "- **Validation Evaluation**:\n",
    "  - After training, the model's performance is evaluated on the validation set.\n",
    "  - Validation accuracy is returned as the objective to be maximized by Optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4efd84e-fed3-4ff5-972d-a70651b37d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d36b81-12d8-4322-81d4-fd42d9de77d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna Objective Function\n",
    "def objective(trial, model_type, train_dataset, val_dataset, device):\n",
    "    # Suggested hyperparameters\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 1e-3, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"SGD\"])\n",
    "\n",
    "    # Optional dropout (for CNN only)\n",
    "    if model_type == \"cnn\":\n",
    "        dropout = trial.suggest_float(\"dropout\", 0.3, 0.6)\n",
    "        model = SimpleCNN(dropout_rate=dropout).to(device)\n",
    "    elif model_type == \"resnet\":\n",
    "        model = build_resnet50().to(device)\n",
    "    elif model_type == \"efficientnet\":\n",
    "        model = build_efficientnet_b0().to(device)\n",
    "\n",
    "    # Optimizer setup\n",
    "    if optimizer_name == \"Adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    else:\n",
    "        momentum = trial.suggest_float(\"momentum\", 0.8, 0.99)\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "    # Data loaders with variable batch size\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Training loop (short version)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Validation accuracy\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    return correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c9f380-de15-4a00-889f-633cc5c59ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Optuna Study for Each Model\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "for model_type in ['cnn', 'resnet', 'efficientnet']:\n",
    "    print(f\"\\n🔍 Tuning {model_type.upper()} model with Optuna...\")\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(lambda trial: objective(trial, model_type, train_dataset, val_dataset, device), n_trials=20)\n",
    "\n",
    "    print(f\"🏆 Best {model_type} trial:\")\n",
    "    print(\"  Accuracy:\", study.best_value)\n",
    "    print(\"  Params:\", study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b40e060-5346-44e5-867d-253fb05c6672",
   "metadata": {},
   "source": [
    "#### *Final Optuna Tuning Results (After Expanding Search Space)*\n",
    "\n",
    "To improve model performance, additional hyperparameters were tuned for each model:\n",
    "- **All models**: `learning_rate`, `weight_decay`, `batch_size`, `optimizer`\n",
    "- **CNN only**: `dropout` (specific to the architecture)\n",
    "- **ResNet50 & EfficientNet-B0 with SGD only**: `momentum`\n",
    "\n",
    "Each model was tuned over 20 trials. Below are the best results obtained from the Optuna studies:\n",
    "\n",
    "##### Best CNN Trial\n",
    "- **Accuracy**: `0.8778`\n",
    "- **Best Parameters**:\n",
    "  - `lr`: `0.00043978`  \n",
    "  - `weight_decay`: `4.73e-05`  \n",
    "  - `batch_size`: `16`  \n",
    "  - `optimizer`: `Adam`  \n",
    "  - `dropout`: `0.4697`\n",
    "\n",
    "##### Best ResNet50 Trial\n",
    "- **Accuracy**: `0.8963`\n",
    "- **Best Parameters**:\n",
    "  - `lr`: `0.00587473`  \n",
    "  - `weight_decay`: `1.80e-05`  \n",
    "  - `batch_size`: `64`  \n",
    "  - `optimizer`: `SGD`  \n",
    "  - `momentum`: `0.8495`\n",
    "\n",
    "##### Best EfficientNet-B0 Trial\n",
    "- **Accuracy**: `0.8689`\n",
    "- **Best Parameters**:\n",
    "  - `lr`: `0.00271083`  \n",
    "  - `weight_decay`: `3.25e-05`  \n",
    "  - `batch_size`: `16`  \n",
    "  - `optimizer`: `SGD`  \n",
    "  - `momentum`: `0.9129`\n",
    "\n",
    "These results indicate that **ResNet50** achieved the best validation accuracy, followed by **EfficientNet-B0** and then **CNN**. The expanded hyperparameter search helped uncover better configurations, particularly for SGD-based models where momentum played a significant role.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af31455b-2f96-4a63-b8e7-7aa0e2716ca3",
   "metadata": {},
   "source": [
    "### 4.2.3 Retrain the Model with Best Hyperparameters\n",
    "This code block retrains each model from scratch using the full training dataset (`train_dataset`, not just `train_loader` from the split) and the best hyperparameters found via Optuna. It includes both training and validation phases and logs loss values for each epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389c5f07-db68-4b77-94c8-b6b02f8db301",
   "metadata": {},
   "source": [
    "**1. CNN**\n",
    "\n",
    "#### Data Loaders\n",
    "- `train_loader` and `val_loader` load the training and validation datasets respectively using the best batch size (`16`).\n",
    "\n",
    "#### Model Definition\n",
    "- `SimpleCNN` is initialized with a dropout rate of `0.4697` and moved to the appropriate device (GPU).\n",
    "\n",
    "#### Optimizer and Loss\n",
    "- Uses **Adam** optimizer with the best learning rate (`0.00043978`) and weight decay (`4.73e-05`).\n",
    "- Applies **CrossEntropyLoss**, which is commonly used for multi-class classification problems.\n",
    "\n",
    "#### Training Loop (10 Epochs)\n",
    "Each epoch involves:\n",
    "\n",
    "- **Training Phase:**\n",
    "  - Sets the model to training mode.\n",
    "  - Iterates through mini-batches:\n",
    "    - Performs a forward pass.\n",
    "    - Computes loss.\n",
    "    - Backpropagates the error.\n",
    "    - Updates weights using the optimizer.\n",
    "  - Accumulates and stores average training loss.\n",
    "\n",
    "- **Validation Phase:**\n",
    "  - Sets the model to evaluation mode.\n",
    "  - Disables gradient computation.\n",
    "  - Runs forward passes on validation data.\n",
    "  - Calculates and stores average validation loss.\n",
    "\n",
    "#### Logging and Monitoring\n",
    "- After every epoch, both training and validation losses are printed.\n",
    "- The losses are stored in `cnn_train_losses` and `cnn_val_losses` for further visualization.\n",
    "- These curves help diagnose **overfitting** (if val loss increases while train loss decreases) or **underfitting** (if both remain high).\n",
    "\n",
    "This setup ensures proper monitoring of model performance and generalization capability during retraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17812f70-44ef-4dd6-ae91-82a5a9e30cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9421a56f-537d-453c-a04b-749977ac5c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best CNN Hyperparameters\n",
    "cnn_params = {\n",
    "    'lr': 0.00043978,\n",
    "    'weight_decay': 4.73e-05,\n",
    "    'batch_size': 16,\n",
    "    'dropout': 0.4697,\n",
    "    'optimizer': 'Adam'\n",
    "}\n",
    "\n",
    "# Define loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=cnn_params['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=cnn_params['batch_size'], shuffle=False)\n",
    "\n",
    "# Define model\n",
    "cnn_model = SimpleCNN(dropout_rate=cnn_params['dropout']).to(device)\n",
    "\n",
    "# Optimizer\n",
    "if cnn_params['optimizer'] == 'Adam':\n",
    "    optimizer = optim.Adam(cnn_model.parameters(), lr=cnn_params['lr'], weight_decay=cnn_params['weight_decay'])\n",
    "else:\n",
    "    optimizer = optim.SGD(cnn_model.parameters(), lr=cnn_params['lr'], weight_decay=cnn_params['weight_decay'])\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize lists to store training and validation losses\n",
    "cnn_train_losses = []\n",
    "cnn_val_losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    # ---- Training ----\n",
    "    cnn_model.train()\n",
    "    running_train_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = cnn_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_train_loss += loss.item()\n",
    "    epoch_train_loss = running_train_loss / len(train_loader)\n",
    "    cnn_train_losses.append(epoch_train_loss)\n",
    "\n",
    "    # ---- Validation ----\n",
    "    cnn_model.eval()\n",
    "    running_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = cnn_model(inputs)\n",
    "            val_loss = criterion(outputs, labels)\n",
    "            running_val_loss += val_loss.item()\n",
    "    epoch_val_loss = running_val_loss / len(val_loader)\n",
    "    cnn_val_losses.append(epoch_val_loss)\n",
    "\n",
    "    # ---- Logging ----\n",
    "    print(f\"[CNN] Epoch {epoch+1} - Train Loss: {epoch_train_loss:.4f} | Val Loss: {epoch_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1afdce-4557-4d5c-b27b-c589a5d90fcb",
   "metadata": {},
   "source": [
    "**2. ResNet50**\n",
    "\n",
    "#### Data Loaders\n",
    "- `train_loader` and `val_loader` are initialized using the best batch size (`64`).\n",
    "- These loaders feed images to the model in mini-batches for training and validation.\n",
    "\n",
    "#### Model Definition\n",
    "- A pre-trained `ResNet50` model is loaded using `torchvision.models`.\n",
    "- All layers are **frozen** (`requires_grad=False`) to retain learned ImageNet features.\n",
    "- The final classification layer (`fc`) is **replaced** with a new fully connected layer to output 6 classes.\n",
    "\n",
    "#### Optimizer and Loss Function\n",
    "- **SGD** optimizer is used (as per best params) with:\n",
    "  - Learning rate = `0.0058747`\n",
    "  - Momentum = `0.8495`\n",
    "  - Weight decay = `1.8e-05`\n",
    "- **CrossEntropyLoss** is used, suitable for multi-class classification.\n",
    "\n",
    "#### Training Loop (10 Epochs)\n",
    "Each epoch has two phases:\n",
    "\n",
    "- **Training Phase:**\n",
    "  - Sets model to training mode.\n",
    "  - Performs forward pass, loss calculation, backpropagation, and weight updates.\n",
    "  - Accumulates average training loss.\n",
    "\n",
    "- **Validation Phase:**\n",
    "  - Sets model to evaluation mode (disables dropout, batch norm updates).\n",
    "  - Disables gradient tracking for efficiency.\n",
    "  - Computes average validation loss.\n",
    "\n",
    "#### Logging and Monitoring\n",
    "- At each epoch, prints both training and validation losses.\n",
    "- Stores losses in `resnet_train_losses` and `resnet_val_losses` lists.\n",
    "- These values can later be plotted to detect **Overfitting**, **Underfitting** or **Good fit**.\n",
    "\n",
    "This process ensures that the model's generalization ability is tracked as it learns with the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44b829e-d369-49e1-a50f-a8b3e050327b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best ResNet Hyperparameters\n",
    "resnet_params = {\n",
    "    'lr': 0.0058747,\n",
    "    'weight_decay': 1.8e-05,\n",
    "    'batch_size': 64,\n",
    "    'optimizer': 'SGD',\n",
    "    'momentum': 0.8495\n",
    "}\n",
    "\n",
    "# Define loaders (use train/val split if available)\n",
    "train_loader = DataLoader(train_dataset, batch_size=resnet_params['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=resnet_params['batch_size'], shuffle=False)\n",
    "\n",
    "# Load pre-trained ResNet\n",
    "resnet_model = models.resnet50(pretrained=True)\n",
    "for param in resnet_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace final layer\n",
    "in_features = resnet_model.fc.in_features\n",
    "resnet_model.fc = nn.Linear(in_features, 6)  # 6 classes\n",
    "resnet_model = resnet_model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "if resnet_params['optimizer'] == 'SGD':\n",
    "    optimizer = optim.SGD(resnet_model.parameters(), lr=resnet_params['lr'],\n",
    "                          momentum=resnet_params['momentum'], weight_decay=resnet_params['weight_decay'])\n",
    "else:\n",
    "    optimizer = optim.Adam(resnet_model.parameters(), lr=resnet_params['lr'],\n",
    "                           weight_decay=resnet_params['weight_decay'])\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Lists to store losses\n",
    "resnet_train_losses = []\n",
    "resnet_val_losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    # ---- Training ----\n",
    "    resnet_model.train()\n",
    "    running_train_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = resnet_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_train_loss += loss.item()\n",
    "    epoch_train_loss = running_train_loss / len(train_loader)\n",
    "    resnet_train_losses.append(epoch_train_loss)\n",
    "\n",
    "    # ---- Validation ----\n",
    "    resnet_model.eval()\n",
    "    running_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = resnet_model(inputs)\n",
    "            val_loss = criterion(outputs, labels)\n",
    "            running_val_loss += val_loss.item()\n",
    "    epoch_val_loss = running_val_loss / len(val_loader)\n",
    "    resnet_val_losses.append(epoch_val_loss)\n",
    "\n",
    "    # ---- Logging ----\n",
    "    print(f\"[ResNet] Epoch {epoch+1} - Train Loss: {epoch_train_loss:.4f} | Val Loss: {epoch_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623ea05a-9a98-4180-b61e-29cf3c6f95e6",
   "metadata": {},
   "source": [
    "**3. EfficientNet-B0**\n",
    "\n",
    "#### Data Loaders\n",
    "- `train_loader` and `val_loader` are created using the best batch size (`16`).\n",
    "- These feed the training and validation datasets into the model during training.\n",
    "\n",
    "#### Model Definition\n",
    "- A pre-trained `EfficientNet-B0` model is loaded using `torchvision.models`.\n",
    "- The **feature extractor layers are frozen** to retain previously learned visual features from ImageNet.\n",
    "- The classifier head is **replaced** with a fully connected layer that outputs predictions for 6 classes.\n",
    "\n",
    "#### Optimizer and Loss Function\n",
    "- **SGD** optimizer is used (based on best parameters) with:\n",
    "  - Learning rate = `0.0027108`\n",
    "  - Momentum = `0.9129`\n",
    "  - Weight decay = `3.25e-05`\n",
    "- **CrossEntropyLoss** is used for multi-class classification.\n",
    "\n",
    "#### Training Loop (10 Epochs)\n",
    "Each epoch consists of:\n",
    "\n",
    "- **Training Phase:**\n",
    "  - Sets model to training mode.\n",
    "  - Performs forward pass, computes loss, backpropagates, and updates weights.\n",
    "  - Accumulates average training loss.\n",
    "\n",
    "- **Validation Phase:**\n",
    "  - Disables gradient computation for efficiency.\n",
    "  - Sets model to evaluation mode (no dropout, no batch norm updates).\n",
    "  - Computes validation loss over the validation set.\n",
    "\n",
    "#### Logging and Monitoring\n",
    "- Logs and prints train and validation loss after each epoch.\n",
    "- Losses are stored in `efficientnet_train_losses` and `efficientnet_val_losses` for visualization.\n",
    "- These curves help identify Overfitting, Underfitting or Good fit.\n",
    "\n",
    "This setup ensures efficient fine-tuning of EfficientNet while monitoring performance across epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09833955-e401-4bf3-8323-449f236e6913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best EfficientNet Hyperparameters\n",
    "efficientnet_params = {\n",
    "    'lr': 0.0027108,\n",
    "    'weight_decay': 3.25e-05,\n",
    "    'batch_size': 16,\n",
    "    'optimizer': 'SGD',\n",
    "    'momentum': 0.9129\n",
    "}\n",
    "\n",
    "# DataLoaders (use val_dataset if available)\n",
    "train_loader = DataLoader(train_dataset, batch_size=efficientnet_params['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=efficientnet_params['batch_size'], shuffle=False)\n",
    "\n",
    "# Load pretrained EfficientNet-B0\n",
    "efficientnet_model = models.efficientnet_b0(pretrained=True)\n",
    "for param in efficientnet_model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace classifier\n",
    "in_features = efficientnet_model.classifier[1].in_features\n",
    "efficientnet_model.classifier[1] = nn.Linear(in_features, 6)  # 6 classes\n",
    "efficientnet_model = efficientnet_model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "if efficientnet_params['optimizer'] == 'SGD':\n",
    "    optimizer = optim.SGD(efficientnet_model.parameters(),\n",
    "                          lr=efficientnet_params['lr'],\n",
    "                          momentum=efficientnet_params['momentum'],\n",
    "                          weight_decay=efficientnet_params['weight_decay'])\n",
    "else:\n",
    "    optimizer = optim.Adam(efficientnet_model.parameters(),\n",
    "                           lr=efficientnet_params['lr'],\n",
    "                           weight_decay=efficientnet_params['weight_decay'])\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Store loss per epoch\n",
    "efficientnet_train_losses = []\n",
    "efficientnet_val_losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    # --- Training ---\n",
    "    efficientnet_model.train()\n",
    "    running_train_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = efficientnet_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_train_loss += loss.item()\n",
    "    epoch_train_loss = running_train_loss / len(train_loader)\n",
    "    efficientnet_train_losses.append(epoch_train_loss)\n",
    "\n",
    "    # --- Validation ---\n",
    "    efficientnet_model.eval()\n",
    "    running_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = efficientnet_model(inputs)\n",
    "            val_loss = criterion(outputs, labels)\n",
    "            running_val_loss += val_loss.item()\n",
    "    epoch_val_loss = running_val_loss / len(val_loader)\n",
    "    efficientnet_val_losses.append(epoch_val_loss)\n",
    "\n",
    "    # --- Logging ---\n",
    "    print(f\"[EfficientNet] Epoch {epoch+1} - Train Loss: {epoch_train_loss:.4f} | Val Loss: {epoch_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7305530-f0a1-40ab-beb9-d595dbdfea53",
   "metadata": {},
   "source": [
    "# 5.0 Model Evaluation and Performance Analysis\n",
    "## 5.1 Evaluation Metrics Used\n",
    "The image classification models were evaluated using the following metrics:\n",
    "- Accuracy: Proportion of correctly classified images.\n",
    "- Precision: Ability of the model to return only relevant images per class.\n",
    "- Recall: Ability of the model to find all relevant images per class.\n",
    "- F1-Score: Harmonic mean of precision and recall.\n",
    "- Confusion Matrix: Visual breakdown of predicted vs. true labels.\n",
    "\n",
    "These metrics are appropriate for multi-class classification, providing both overall and class-wise performance insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3eaa70c-facb-453d-b80a-cb9ad9172483",
   "metadata": {},
   "source": [
    "### 5.1.1 Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef567287-9872-47de-8093-2721d88ae70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74540d6f-aa9f-494e-9b4f-bb61d13ca241",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, class_names, model_name):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    print(f\"✅ {model_name} Accuracy: {acc:.4f}\")\n",
    "    print(f\"\\n📋 {model_name} Classification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=class_names))\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(f\"{model_name} Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "    return acc, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07bd0fc-517e-44b4-a61b-07bed18d1eac",
   "metadata": {},
   "source": [
    "### 5.1.2 Apply Evaluation on Best Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e032a97-eeb4-482f-9dde-33601296d35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_acc, cnn_f1 = evaluate_model(cnn_model, test_loader, class_names, model_name=\"CNN\")\n",
    "resnet_acc, resnet_f1 = evaluate_model(resnet_model, test_loader, class_names, model_name=\"ResNet\")\n",
    "efficientnet_acc, efficientnet_f1 = evaluate_model(efficientnet_model, test_loader, class_names, model_name=\"EfficientNet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6a32ef-71af-44d4-b179-8076e4cdc29d",
   "metadata": {},
   "source": [
    "### Accuracy & Macro-Averaged F1-Score\n",
    "\n",
    "| Model        | Accuracy | Macro F1-Score |\n",
    "|--------------|----------|----------------|\n",
    "| CNN          | 86.30%   | 0.86           |\n",
    "| ResNet50     | 87.83%   | 0.88           |\n",
    "| EfficientNet | 84.73%   | 0.85           |\n",
    "\n",
    "- **ResNet** achieved the highest accuracy and macro F1-score, indicating better generalization and class-wise balance.\n",
    "- **CNN** performs decently but lags slightly behind ResNet in most metrics.\n",
    "- **EfficientNet** shows good precision but has slightly lower recall and accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "### Confusion Matrix Insights\n",
    "\n",
    "- **CNN**:\n",
    "  - Performs very well on `forest` (96% recall) and `street` (88% F1-score).\n",
    "  - Struggles slightly with `glacier` (F1 = 0.81) with some misclassifications into `sea` and `mountain`. meanwhile, `mountain` (F1 = 0.83), with some misclassifications into `glacier` and `sea`.\n",
    "\n",
    "- **ResNet**:\n",
    "  - Near-perfect recall on `forest` (99%), `street` and `buildings` (91%).\n",
    "  - Major improvement on `buildings` compared to CNN.\n",
    "  - `Sea` class has slightly lower precision (0.79) due to confusion with `glacier` and `mountain`.\n",
    "\n",
    "- **EfficientNet**:\n",
    "  - Performs best on `forest` and `sea` (high recall and precision).\n",
    "  - Noticeable drop in performance for `street` (only 80% recall), often misclassified as `buildings`.\n",
    "  - Also more confusion between `glacier` and `mountain`, as seen by higher off-diagonal values.\n",
    "\n",
    "---\n",
    "\n",
    "### Final Remarks\n",
    "- All models struggled at differentiating between `glacier` vs `mountain` vs `sea`, as well as `buildings` vs `street`, likely due to visual similarities in the images from these classes.\n",
    "- **Best overall model**: `ResNet` due to consistent performance across all classes.\n",
    "- **Most lightweight**: `CNN`, suitable for faster inference if slight accuracy trade-off is acceptable.\n",
    "- **EfficientNet** may benefit from unfreezing more layers or longer training, as it currently underperforms despite its powerful architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f190bb-fbf9-4e1e-ba4b-e4a0fe26ff86",
   "metadata": {},
   "source": [
    "## 5.2 Visualizing Trends\n",
    "### 5.2.1 Model Comparison: Accuracy & F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbb45e5-865e-4cba-b438-887a601d7942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['CNN', 'ResNet', 'EfficientNet'],\n",
    "    'Accuracy': [cnn_acc, resnet_acc, efficientnet_acc],\n",
    "    'F1-Score': [cnn_f1, resnet_f1, efficientnet_f1]\n",
    "})\n",
    "\n",
    "results.plot(kind='bar', x='Model', figsize=(8,5), legend=True)\n",
    "plt.title(\"Model Comparison: Accuracy and F1-Score\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.grid(axis='y')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b009b46-3166-442e-95c4-1ddb41287471",
   "metadata": {},
   "source": [
    "The bar chart above compares the performance of three models; CNN, ResNet, and EfficientNet based on two metrics: Accuracy and F1-Score.\n",
    "\n",
    "ResNet achieved the highest performance overall, with both accuracy and F1-score around 0.88, indicating strong and balanced classification across all classes.\n",
    "\n",
    "CNN performed slightly below ResNet, with an accuracy and F1-score of approximately 0.86. While it is a simpler model, it still managed competitive results.\n",
    "\n",
    "EfficientNet scored the lowest among the three, with both accuracy and F1-score around 0.85. This suggests that while EfficientNet is generally powerful, it may have underperformed due to freezing too many layers or insufficient fine-tuning in this context.\n",
    "\n",
    "> ResNet not only delivered the highest accuracy but also maintained strong F1-score consistency, making it the most reliable model for this image classification task. However, the margin of difference among the models is relatively small, showing that all three models are capable but may benefit from further tuning or deeper fine-tuning strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43703b80-67eb-4051-b6d2-2d7eb1994448",
   "metadata": {},
   "source": [
    "### 5.2.2 Plotting Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e608f5-27b8-4545-a37b-b3e8355e2acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# CNN\n",
    "axes[0].plot(cnn_train_losses, label='Train Loss', linestyle='--')\n",
    "axes[0].plot(cnn_val_losses, label='Val Loss', linestyle='-')\n",
    "axes[0].set_title('CNN Loss Curve')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# ResNet\n",
    "axes[1].plot(resnet_train_losses, label='Train Loss', linestyle='--')\n",
    "axes[1].plot(resnet_val_losses, label='Val Loss', linestyle='-')\n",
    "axes[1].set_title('ResNet50 Loss Curve')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "# EfficientNet\n",
    "axes[2].plot(efficientnet_train_losses, label='Train Loss', linestyle='--')\n",
    "axes[2].plot(efficientnet_val_losses, label='Val Loss', linestyle='-')\n",
    "axes[2].set_title('EfficientNet-B0 Loss Curve')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Loss')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True)\n",
    "\n",
    "plt.suptitle('Training vs Validation Loss Curves for All Models', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af3e9fd-010c-44d8-bc5d-460e1aa1c62d",
   "metadata": {},
   "source": [
    "The figure above presents the **training vs validation loss** curves across 10 epochs for the three models: **CNN**, **ResNet50**, and **EfficientNet-B0**.\n",
    "\n",
    "Typically, validation loss is often higher than training loss curves ([source](https://www.youtube.com/watch?v=p3CcfIjycBA)) because:\n",
    "1. Training data is seen repeatedly, so the model becomes good at minimizing loss on it.\n",
    "2. Validation data is unseen, so performance tends to be worse (higher loss).\n",
    "3. Data augmentation (`transform_train`) introduces randomness in training that helps generalize, but the clean validation set still differs.\n",
    "4. Overfitting is more likely with image data if the model is large and the dataset is small.\n",
    "\n",
    "However, all 3 models shown have validation loss curve lower than the training loss, because:\n",
    "1. for the CNN model, dropout was applied (`cnn_model = SimpleCNN(dropout_rate=cnn_params['dropout']).to(device)`). Dropout randomly \"drops\" (disables) a fraction of neurons in the network during training only. But, during validation (and testing), dropout is turned off, so, all neurons are active. This results in lower loss on validation data than on dropout-weakened training data.\n",
    "2. In both ResNet and EfficientNet models, the majority of layers are frozen to retain pretrained ImageNet features. As a result, only the final classification layer is being trained. The pretrained layers already generalize well to visual features, benefiting the validation set more than the training set.\n",
    "3. The training set uses data augmentation to improve generalization (`transforms.RandomHorizontalFlip(), transforms.RandomRotation(15)`). These augmentations introduce variability, making the training task harder. The validation set is cleaner (no augmentations), leading to lower loss.\n",
    "\n",
    "---\n",
    "\n",
    "**CNN Loss Curve**:\n",
    "- The training loss steadily decreases across epochs, indicating effective learning.\n",
    "- Validation loss closely follows the training trend and improves consistently.\n",
    "- No sign of overfitting is observed, as both curves appears smoooth and almost converge in the end, showing generalization is well maintained.\n",
    "\n",
    "**ResNet50 Loss Curve**:\n",
    "- ResNet50 starts with a low loss and continues to converge quickly within the first few epochs.\n",
    "- Validation loss aligns well with training loss, with minor fluctuations suggesting good generalization.\n",
    "- Slight gap between the curves indicates minimal overfitting, and the model maintains stable performance throughout.\n",
    "\n",
    "**EfficientNet-B0 Loss Curve**:\n",
    "- Shows the slowest improvement in training loss across epochs compared to CNN and ResNet.\n",
    "- Validation loss improves rapidly and flattens, remaining consistently below training loss.\n",
    "- The persistent gap between training and validation losses may indicate **underfitting**, possibly due to freezing too many layers or conservative training.\n",
    "\n",
    "---\n",
    "\n",
    "- Among the three models, **ResNet50** exhibits the most stable and flattened loss curves, suggesting consistent learning. While **none of the models demonstrate a clear convergence**, the training and validation losses maintain a small and stable gap, indicating steady progress.\n",
    "- **CNN** also performs well and steadily improves, suitable for lightweight deployment.\n",
    "- For **CNN** and **ResNet50**, extending training over more epochs could help determine whether the curves will converge or begin to diverge, offering better insight into their long-term learning behavior.\n",
    "- In contrast, **EfficientNet-B0** shows a noticeably large and persistent gap between training and validation loss, which indicates underfitting. The model is potentially too constrained or insufficiently trained for the task and may require **unfreezing more layers** or **more epochs** to fully leverage its capacity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c720a11d-c20a-4a48-952b-ec8199090ca8",
   "metadata": {},
   "source": [
    "## 5.3 Prediction Samples\n",
    "This code block performs inference using three trained models (CNN, ResNet, EfficientNet) on images stored in the `seg_pred` directory.\n",
    "\n",
    "- The model names are defined, corresponding model objects, class labels, and the image transform (`transform_test`) used for evaluation.\n",
    "- The prediction directory path is specified.\n",
    "\n",
    "The loop then:\n",
    "1. Iterates over each model.\n",
    "2. Loads and transforms each image (ignoring subdirectories).\n",
    "3. Runs the model in evaluation mode without computing gradients.\n",
    "4. Predicts the image class and maps the output index to the corresponding class name.\n",
    "5. Stores all predictions and saves them into a CSV file for each model (`seg_pred_predictions_xx.csv`).\n",
    "\n",
    "This modular code allows batch predictions and comparisons across different models on the same set of unseen images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7139e1-1945-497c-bd63-24518bf18469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of models and their names\n",
    "model_names = ['CNN', 'ResNet', 'EfficientNet']\n",
    "models_list = [cnn_model, resnet_model, efficientnet_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f914162c-07e9-4c13-89c1-b7d1b0ce7d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class names\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8cb219-58e9-4e2f-a39b-e9c89a14f7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform\n",
    "transform_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a89a797-6510-41d3-95a3-51135561f684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict images in seg_pred\n",
    "pred_dir = r\"C:\\Users\\user\\OneDrive - Universiti Teknologi Malaysia (UTM)\\MRTB2153 Advanced Artificial Intelligence\\PMA\\Intel Image Classification\\seg_pred\\seg_pred\"\n",
    "\n",
    "# Evaluate each model\n",
    "all_results = {}\n",
    "\n",
    "for model_name, model in zip(model_names, models_list):\n",
    "    print(f\"🔍 Predicting with {model_name}...\")\n",
    "    model.eval()\n",
    "    predicted_labels = []\n",
    "    results = []\n",
    "\n",
    "    for img_name in os.listdir(pred_dir):\n",
    "        img_path = os.path.join(pred_dir, img_name)\n",
    "        if os.path.isdir(img_path): continue\n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        input_tensor = transform_test(img).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "            pred_class = torch.argmax(output, dim=1).item()\n",
    "            predicted_label = class_names[pred_class]\n",
    "            predicted_labels.append(predicted_label)\n",
    "            results.append({'Filename': img_name, 'Predicted_Class': predicted_label})\n",
    "\n",
    "    all_results[model_name] = results  # Save for second loop\n",
    "\n",
    "    # Save CSV immediately\n",
    "    df = pd.DataFrame(results)\n",
    "    csv_path = f\"seg_pred_predictions_{model_name.lower()}.csv\"\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"✅ Saved predictions to {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60c0365-68fd-40cf-93bc-0dfb3fa1b32f",
   "metadata": {},
   "source": [
    "### 5.3.1 Plot & Visualize Predictions by Class\n",
    "See how many images were predicted as each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35b159c-88e6-4034-aaa8-1bb01ff66072",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "for model_name in model_names:\n",
    "    results = all_results[model_name]\n",
    "    predicted_labels = [r['Predicted_Class'] for r in results]\n",
    "\n",
    "    # 1. Plot class distribution\n",
    "    counter = Counter(predicted_labels)\n",
    "    plt.figure(figsize=(8,5))\n",
    "    sns.barplot(x=list(counter.keys()), y=list(counter.values()))\n",
    "    plt.title(f\"📊 Predicted Class Distribution - {model_name}\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 2. Show sample predictions\n",
    "    sample_results = results[:30]  \n",
    "    n_samples = len(sample_results)\n",
    "    n_cols = 3\n",
    "    n_rows = math.ceil(n_samples / n_cols)\n",
    "    \n",
    "    fig, axs = plt.subplots(n_rows, n_cols, figsize=(12, 4 * n_rows))\n",
    "    \n",
    "    # If axs is 1D (for n_rows == 1), reshape for consistency\n",
    "    axs = np.array(axs).reshape(n_rows, n_cols)\n",
    "    \n",
    "    for i, row in enumerate(sample_results):\n",
    "        img_path = os.path.join(pred_dir, row['Filename'])\n",
    "        img = Image.open(img_path)\n",
    "        axs[i//n_cols, i%n_cols].imshow(img)\n",
    "        axs[i//n_cols, i%n_cols].set_title(row['Predicted_Class'])\n",
    "        axs[i//n_cols, i%n_cols].axis('off')\n",
    "    \n",
    "    # Hide unused axes\n",
    "    for j in range(i + 1, n_rows * n_cols):\n",
    "        axs[j // n_cols, j % n_cols].axis('off')\n",
    "    \n",
    "    plt.suptitle(f\"🔍 Sample Predictions - {model_name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfef17a3-fd3b-4f3f-bf07-e4131f7e8260",
   "metadata": {},
   "source": [
    "#### *Bar Chart interpretation*\n",
    "**CNN Predictions on `seg_pred`**\n",
    "\n",
    "- CNN predictions are fairly balanced, though there is a **slight bias toward the `mountain` class** (most frequently predicted).\n",
    "- The prediction for other classes appear to be more balanced. \n",
    "\n",
    "---\n",
    "\n",
    "**ResNet Predictions on `seg_pred`**\n",
    "\n",
    "- ResNet shows a more **pronounced class imbalance**:\n",
    "  - Overpredicts `glacier` and `sea`\n",
    "  - Underpredicts `mountain`\n",
    "- This may indicate that ResNet is more sensitive to certain low-level features in `glacier` and `sea`, and tends to **confuse `mountain` with visually similar classes** like `glacier` or `forest`.\n",
    "\n",
    "---\n",
    "\n",
    "**EfficientNet Predictions on `seg_pred`**\n",
    "\n",
    "- EfficientNet demonstrates the **most balanced class distribution** among all three models.\n",
    "- Slightly favors `mountain` and `buildings`, but the differences across classes are minimal.\n",
    "- This reflects **stronger generalization capabilities**, likely due to EfficientNet's compound scaling and deeper architecture.\n",
    "\n",
    "---\n",
    "\n",
    "| Model          | Most Predicted Class     | Least Predicted Class | Notes                                      |\n",
    "|----------------|---------------------------|------------------------|---------------------------------------------|\n",
    "| **CNN**        | Mountain                  | Glacier                | Slight preference toward natural scenes     |\n",
    "| **ResNet**     | Glacier, Sea              | Mountain               | Confuses mountain with glacier/forest       |\n",
    "| **EfficientNet** | Mountain, Buildings      | Street                 | Most balanced; good generalization          |\n",
    "\n",
    "---\n",
    "\n",
    "These prediction patterns reveal each model’s tendencies, biases, and generalization strengths. Even though all were trained on a balanced dataset, differences in **architecture depth**, **feature extraction**, and **inductive biases** shape how each model performs on unseen data like `seg_pred`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2315ca95-2d32-4154-9b7d-ce1b76345461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of 1-based indices to visualize (not in order)\n",
    "selected_indices = [1, 4, 5, 10, 13, 16, 30, 24]\n",
    "selected_indices = [i - 1 for i in selected_indices]  # Convert to 0-based indexing\n",
    "\n",
    "# Plotting setup\n",
    "n_samples = len(selected_indices)\n",
    "n_cols = 4\n",
    "n_rows = math.ceil(n_samples / n_cols)\n",
    "\n",
    "fig, axs = plt.subplots(n_rows, n_cols, figsize=(16, 4 * n_rows))\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Loop through each selected image index\n",
    "for plot_idx, sample_idx in enumerate(selected_indices):\n",
    "    row = sample_results[sample_idx]\n",
    "    img_path = os.path.join(pred_dir, row['Filename'])\n",
    "    img = Image.open(img_path)\n",
    "\n",
    "    # Get predicted labels from each model\n",
    "    filename = row['Filename']\n",
    "    pred_cnn = next((r['Predicted_Class'] for r in all_results['CNN'] if r['Filename'] == filename), \"N/A\")\n",
    "    pred_resnet = next((r['Predicted_Class'] for r in all_results['ResNet'] if r['Filename'] == filename), \"N/A\")\n",
    "    pred_efficientnet = next((r['Predicted_Class'] for r in all_results['EfficientNet'] if r['Filename'] == filename), \"N/A\")\n",
    "\n",
    "    # Display image and predictions\n",
    "    axs[plot_idx].imshow(img)\n",
    "    axs[plot_idx].axis('off')\n",
    "    axs[plot_idx].set_title(\n",
    "        f\"📷 {filename}\\n\"\n",
    "        f\"CNN: {pred_cnn}\\n\"\n",
    "        f\"ResNet: {pred_resnet}\\n\"\n",
    "        f\"EffNet: {pred_efficientnet}\",\n",
    "        fontsize=9\n",
    "    )\n",
    "\n",
    "# Hide any remaining empty subplots\n",
    "for i in range(len(selected_indices), len(axs)):\n",
    "    axs[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabdc3d8-43d4-418e-b38c-2870e128cc94",
   "metadata": {},
   "source": [
    "Based on the first 30 predicted image samples, CNN demonstrated perfect prediction accuracy, correctly classifying all images. In contrast, ResNet50 and EfficientNet showed occasional misclassifications, indicating they struggled with some of the samples.\n",
    "\n",
    "The 8 images displayed above were selected to highlight instances where ResNet and EfficientNet misclassified the input images.\n",
    "\n",
    "#### *Visual Analysis of Selected Image Predictions*\n",
    "\n",
    "The visual analysis of the selected image predictions (from **CNN**, **ResNet**, and **EfficientNet**) reveals a few key patterns and model behaviors:\n",
    "\n",
    "**General Observations**\n",
    "\n",
    "1. **Class Confusion in Urban Scenes**:\n",
    "    - `10004.jpg`, `10040.jpg`, and `10082.jpg` are clearly **urban scenes**.\n",
    "        - `10004.jpg` is more accurately classified as a building, given the absence of visual cues typically associated with a **street**, such as vehicles, pedestrians, or road markings. Among the three models, only the CNN correctly identified this image. \n",
    "        - ResNet and EfficientNet correctly predicted **street** for `10040.jpg`, but ResNet misclassified it as **buildings**.\n",
    "        - Similar to `10004.jpg`, `10082.jpg` lacks key street indicators such as vehicles, pedestrians, or road markings, and is therefore more appropriately classified as a building. However, EfficientNet failed to correctly identify this image. \n",
    "    - These inconsistencies indicate that the model struggles to distinguish between **street** and **buildings**, particularly when both elements may be present but only partially visible, such as when the street portion appears to be cropped out in images primarily showing buildings.\n",
    "\n",
    "2. **Mountain vs Glacier Confusion**:\n",
    "    - `10013.jpg` and `10017.jpg` are mountainous snowy landscapes.\n",
    "        - CNN and ResNet predicted **mountain**, but EfficientNet predicted **glacier** for `10013.jpg`.\n",
    "        - ResNet predicted **glacier** for `10017.jpg`, although the terrain looks more **mountainous**.\n",
    "    - This confusion is likely due to **snowy textures** being present in both glacier and mountain classes, making them visually similar.\n",
    "\n",
    "3. **Scene Misinterpretation by ResNet**:\n",
    "    - `10047.jpg` shows a mountainous scene, with trees.\n",
    "        - CNN and EfficientNet both predict **mountain**, as **trees and background hills** can clearly be seen.\n",
    "        - ResNet surprisingly predicts **sea**, which seems inaccurate.\n",
    "    - `10100.jpg`, another hilly forest scene, is interpreted by ResNet as **sea**, which again suggests a potential misreading of **hazy backgrounds or low-contrast areas**.\n",
    "\n",
    "4. **High Consensus; Glacier**:\n",
    "    - `10054.jpg` has **strong agreement**: all models predicted **glacier**, likely due to the clear presence of ice structures.\n",
    "    - This indicates that **glacier** is a well-learned class for all three models when the ice formations are distinct, despite the presence of a mountain in the background.\n",
    "\n",
    "---\n",
    "\n",
    "### ***Key Takeaways***\n",
    "\n",
    "| Observation Category               | Image(s) Involved         | Misclassification Details                                                                                                                                                   | Likely Cause                                                                                   |\n",
    "|------------------------------------|----------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------|\n",
    "| **Class Confusion in Urban Scenes** | `10004.jpg`, `10040.jpg`, `10082.jpg` | - `10004.jpg`: CNN predicted **buildings** correctly; ResNet & EfficientNet misclassified as **street**.<br>- `10040.jpg`: ResNet misclassified as **buildings**; others predicted **street** correctly.<br>- `10082.jpg`: Only CNN predicted **buildings** correctly; EfficientNet misclassified as **street**. | Lack of street indicators (cars, road, pedestrians) and cropped street views in building scenes cause confusion between **buildings** and **street**. |\n",
    "| **Mountain vs Glacier Confusion**  | `10013.jpg`, `10017.jpg`  | - `10013.jpg`: CNN & ResNet predicted **mountain**; EfficientNet predicted **glacier**.<br>- `10017.jpg`: ResNet misclassified as **glacier**; others predicted **mountain**.                     | Snowy textures present in both **mountain** and **glacier** classes contribute to visual similarity. |\n",
    "| **Scene Misinterpretation by ResNet** | `10047.jpg`, `10100.jpg`   | - `10047.jpg`: CNN & EfficientNet correctly predicted **mountain**; ResNet misclassified as **sea**.<br>- `10100.jpg`: Forested hills misclassified as **sea** by ResNet.                          | Possibly due to **hazy backgrounds** or **low-contrast areas** being misread as water bodies.     |\n",
    "| **High Consensus – Glacier**       | `10054.jpg`                | - All models correctly predicted **glacier**.                                                                                                                               | Distinct ice formations make this an **easily recognizable** glacier scene for all models.        |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591912cf-51a1-49fe-b231-c8d206936ba6",
   "metadata": {},
   "source": [
    "# 6.0 Future Work\n",
    "\n",
    "While this project has successfully demonstrated the effectiveness of CNN, ResNet50, and EfficientNet-B0 in classifying natural scene images from the Intel dataset, there are several areas for potential improvement and future exploration:\n",
    "\n",
    "## 6.1 Fine-Tuning Pretrained Models\n",
    "\n",
    "In this study, both ResNet50 and EfficientNet-B0 had their feature extraction layers frozen during training, with only the classifier layers updated. While this approach speeds up training and reduces overfitting, it may limit the models' ability to adapt to dataset-specific patterns. Future work should explore:\n",
    "- **Unfreezing deeper layers gradually** using a discriminative learning rate schedule.\n",
    "- **Full fine-tuning** for larger training epochs to exploit the full representational power of pretrained weights.\n",
    "\n",
    "## 6.2 Expand Evaluation Metrics\n",
    "\n",
    "Although accuracy,macro F1-score, precision, confusion matrices and recall provided meaningful comparisons, future experiments could benefit from a more comprehensive evaluation by incorporating:\n",
    "- **Class-wise ROC-AUC**, particularly for binary or grouped class tasks\n",
    "- **Model robustness checks** using adversarial examples or image augmentations\n",
    "\n",
    "## 6.3 Data Augmentation and Regularization\n",
    "\n",
    "More advanced data augmentation techniques could improve model generalization, particularly for underrepresented classes such as **glacier** or **street**:\n",
    "- **AutoAugment**, **CutMix**, or **MixUp**\n",
    "- Additional regularization such as **label smoothing**, **dropblock**, or **stochastic depth**\n",
    "\n",
    "## 6.4 Addressing Class Confusion\n",
    "\n",
    "Error analysis revealed frequent misclassification between:\n",
    "- **Buildings vs Street**\n",
    "- **Mountain vs Glacier**\n",
    "\n",
    "Future strategies may include:\n",
    "- **Class-specific data balancing**\n",
    "- **Multi-task learning** to learn shared and distinct features across related classes\n",
    "- **Attention mechanisms** to help models focus on relevant image regions\n",
    "\n",
    "## 6.5 Larger-Scale Evaluation on Unlabelled Data\n",
    "\n",
    "The inference task on 7,301 unlabelled `seg_pred` images provided further insight into real-world generalization. Future efforts should include:\n",
    "- **Manual annotation or semi-supervised labelling** of unlabelled samples to validate predictions.\n",
    "- **Model ensembling** to reduce individual model biases and increase prediction confidence.\n",
    "\n",
    "## 6.6 Deployment Considerations\n",
    "\n",
    "For real-world use:\n",
    "- **Model compression** (pruning, quantization) could reduce inference time.\n",
    "- **On-device deployment testing** for edge or mobile scenarios can be explored, especially for the lightweight CNN model.\n",
    "\n",
    "---\n",
    "\n",
    "Overall, this project sets a strong baseline for natural scene classification using both custom and pretrained deep learning models, while highlighting multiple paths for enhancing accuracy, generalization, and deployment readiness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3de4c4-7029-4451-ade0-5671052a2134",
   "metadata": {},
   "source": [
    "# 7.0 Conclusion\n",
    "\n",
    "Based on the **evaluation using the labeled `seg_test` dataset**, **ResNet50** achieved the highest performance, with both **Accuracy** and **F1-Score** around **0.88**, indicating strong and consistent classification across all six scene classes. **CNN** followed closely with scores around **0.86**, while **EfficientNet-B0** lagged slightly behind at **0.85**, likely due to underfitting caused by freezing too many layers or insufficient fine-tuning.\n",
    "\n",
    "However, when analyzing the **predictions on the 7,301 unlabeled images from the `seg_pred` folder**, the performance dynamics appeared different:\n",
    "\n",
    "- **CNN** produced the most accurate predictions among the **first 30 visually inspected samples**, often correctly classifying difficult cases (distinguishing buildings from streets, and mountains from glaciers). This suggests strong instance-level performance and generalization despite being a simpler model.\n",
    "\n",
    "- **EfficientNet-B0**, although less accurate on those 30 samples, demonstrated the **most balanced class distribution** across the entire `seg_pred` dataset. This indicates that while it may not always predict individual images correctly, it **avoids class bias**, potentially making it better suited for tasks requiring fairness across all categories.\n",
    "\n",
    "- **ResNet50**, despite its top testing performance, showed **some inconsistencies in generalization** during `seg_pred` predictions. While it maintained good accuracy in several cases, it occasionally misclassified scenes (labeling mountainous forests as \"sea\"), suggesting **sensitivity to ambiguous or low-contrast features** in real-world images.\n",
    "\n",
    "---\n",
    "\n",
    "- **ResNet50** is the most **accurate and stable model** when evaluated on labeled test data (`seg_test`).\n",
    "- **CNN** demonstrates **strong generalization** on unseen examples (`seg_pred`) and performs well in practical scenarios.\n",
    "- **EfficientNet-B0** provides the **most balanced class predictions** on a large volume of unlabeled data, though it may require deeper fine-tuning to improve accuracy on specific instances.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary Table: Model Comparison\n",
    "\n",
    "| Model         | Strengths                                                                 | Weaknesses                                                                    | Scalability                    | Suitability                                                           |\n",
    "|---------------|---------------------------------------------------------------------------|--------------------------------------------------------------------------------|--------------------------------|------------------------------------------------------------------------|\n",
    "| **CNN**       | - Fast training and inference time<br>- High accuracy on small sample tests<br>- Simple to implement and tune | - Lower generalization on full dataset<br>- May not capture deep spatial patterns as well as deeper models | Lightweight, deployable on edge devices | Suitable for mobile or resource-constrained environments where model size and speed matter |\n",
    "| **ResNet50**  | - Highest accuracy & F1 score on test set<br>- Stable loss curves<br>- Pretrained backbone improves learning | - Occasional misclassifications on ambiguous scenes<br>- Slightly larger model size | Moderate (requires GPU for efficient training) | Best for general-purpose, balanced performance use cases, especially in production environments |\n",
    "| **EfficientNet-B0** | - More balanced predictions across all 7,301 unlabelled images<br>- Compact yet deep architecture | - Underperformed in test set<br>- Shows signs of underfitting due to excessive freezing | Moderate (higher training time, needs careful fine-tuning) | Ideal for large-scale deployment where uniform prediction distribution is important, but requires tuning |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
